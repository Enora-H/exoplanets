{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "from astropy.stats import sigma_clip\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/axelp/Desktop/Travail/DeepLearning/train/100468857/AIRS-CH0_signal.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/100468857/FGS1_signal.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/100468857/AIRS-CH0_calibration/dark.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/100468857/AIRS-CH0_calibration/dead.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/100468857/AIRS-CH0_calibration/flat.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/100468857/AIRS-CH0_calibration/linear_corr.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/100468857/AIRS-CH0_calibration/read.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/100468857/FGS1_calibration/dark.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/100468857/FGS1_calibration/dead.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/100468857/FGS1_calibration/flat.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/100468857/FGS1_calibration/linear_corr.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/100468857/FGS1_calibration/read.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/1011759019/AIRS-CH0_signal.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/1011759019/FGS1_signal.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/1011759019/AIRS-CH0_calibration/dark.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/1011759019/AIRS-CH0_calibration/dead.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/1011759019/AIRS-CH0_calibration/flat.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/1011759019/AIRS-CH0_calibration/linear_corr.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/1011759019/AIRS-CH0_calibration/read.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/1011759019/FGS1_calibration/dark.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/1011759019/FGS1_calibration/dead.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/1011759019/FGS1_calibration/flat.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/1011759019/FGS1_calibration/linear_corr.parquet',\n",
       " 'C:/Users/axelp/Desktop/Travail/DeepLearning/train/1011759019/FGS1_calibration/read.parquet']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calibration_set = ['M','L','D','F','B']\n",
    "CHUNKS_SIZE = 30\n",
    "\n",
    "path_out = 'C:/Users/axelp/Desktop/Travail/DeepLearning/Preprocessed'\n",
    "path_planets = 'C:/Users/axelp/Desktop/Travail/DeepLearning/train'\n",
    "path_folder = 'C:/Users/axelp/Desktop/Travail/DeepLearning'\n",
    "\n",
    "train_adc_info = pd.read_csv(os.path.join(path_folder, 'train_adc_info.csv'))\n",
    "train_adc_info = train_adc_info.set_index('planet_id')\n",
    "axis_info = pd.read_parquet(os.path.join(path_folder,'axis_info.parquet'))\n",
    "\n",
    "# Fichier contenant toutes les planètes\n",
    "files = [os.path.join(root, file) for root, dirs, files in os.walk(path_planets) for file in files]\n",
    "files = [file.replace('\\\\', '/') for file in files]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fonctions de calibrage\n",
    "\n",
    "def ADC_convert(signal, gain, offset):\n",
    "    signal = signal.astype(np.float64)\n",
    "    signal /= gain\n",
    "    signal += offset\n",
    "    return signal\n",
    "\n",
    "def mask_hot_dead(signal, dead, dark):\n",
    "    hot = sigma_clip(\n",
    "        dark, sigma=5, maxiters=5\n",
    "    ).mask\n",
    "    hot = np.tile(hot, (signal.shape[0], 1, 1))\n",
    "    dead = np.tile(dead, (signal.shape[0], 1, 1))\n",
    "    signal = np.ma.masked_where(dead, signal)\n",
    "    signal = np.ma.masked_where(hot, signal)\n",
    "    return signal\n",
    "\n",
    "def apply_linear_corr(linear_corr,clean_signal):\n",
    "    linear_corr = np.flip(linear_corr, axis=0)\n",
    "    for x, y in itertools.product(\n",
    "                range(clean_signal.shape[1]), range(clean_signal.shape[2])\n",
    "            ):\n",
    "        poli = np.poly1d(linear_corr[:, x, y])\n",
    "        clean_signal[:, x, y] = poli(clean_signal[:, x, y])\n",
    "    return clean_signal\n",
    "\n",
    "def clean_dark(signal, dead, dark, dt):\n",
    "\n",
    "    dark = np.ma.masked_where(dead, dark)\n",
    "    dark = np.tile(dark, (signal.shape[0], 1, 1))\n",
    "\n",
    "    signal -= dark* dt[:, np.newaxis, np.newaxis]\n",
    "    return signal\n",
    "\n",
    "def get_cds(signal):\n",
    "    cds = signal[:,1::2,:,:] - signal[:,::2,:,:]\n",
    "    return cds\n",
    "\n",
    "def bin_obs(cds_signal,binning):\n",
    "    cds_transposed = cds_signal.transpose(0,1,3,2)\n",
    "    cds_binned = np.zeros((cds_transposed.shape[0], cds_transposed.shape[1]//binning, cds_transposed.shape[2], cds_transposed.shape[3]))\n",
    "    for i in range(cds_transposed.shape[1]//binning):\n",
    "        cds_binned[:,i,:,:] = np.sum(cds_transposed[:,i*binning:(i+1)*binning,:,:], axis=1)\n",
    "    return cds_binned\n",
    "\n",
    "def correct_flat_field(flat,dead, signal):\n",
    "    flat = flat.transpose(1, 0)\n",
    "    dead = dead.transpose(1, 0)\n",
    "    flat = np.ma.masked_where(dead, flat)\n",
    "    flat = np.tile(flat, (signal.shape[0], 1, 1))\n",
    "    signal = signal / flat\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Autres fonctions utiles\n",
    "\n",
    "# Découper en CHUNKS et récupérer les indices:\n",
    "def get_index(files, CHUNKS_SIZE):\n",
    "    index = []\n",
    "    for file in files:\n",
    "        file_name = file.split('/')[-1]\n",
    "        if file_name.split('_')[0] == 'AIRS-CH0' and file_name.split('_')[1] == 'signal.parquet':\n",
    "            file_index = os.path.basename(os.path.dirname(file))\n",
    "            index.append(int(file_index))\n",
    "    index = np.array(index)\n",
    "    index = np.sort(index)\n",
    "    if len(index) > CHUNKS_SIZE:\n",
    "        index = np.array_split(index, len(index) // CHUNKS_SIZE)\n",
    "    else:\n",
    "        index = [index]\n",
    "    return index\n",
    "\n",
    "def load_data (file, chunk_size, nb_files) : \n",
    "    data0 = np.load(file + '_0.npy')\n",
    "    data_all = np.zeros((nb_files*chunk_size, data0.shape[1], data0.shape[2], data0.shape[3]))\n",
    "    data_all[:chunk_size] = data0\n",
    "    for i in range (1, nb_files) : \n",
    "        data_all[i*chunk_size:(i+1)*chunk_size] = np.load(file + '_{}.npy'.format(i))\n",
    "    return data_all \n",
    "\n",
    "def def_suffix(calibration_set):\n",
    "    return ''.join(calibration_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = get_index(files, CHUNKS_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing à partir d'un set de calibration\n",
    "\n",
    "def preprocess(path_out, path_folder, calibration_set, CHUNKS_SIZE):\n",
    "\n",
    "    index = get_index(files, CHUNKS_SIZE)\n",
    "\n",
    "    suffix = def_suffix(calibration_set)\n",
    "    os.makedirs(os.path.join(path_out, suffix), exist_ok=True)\n",
    "\n",
    "    DO_MASK = False\n",
    "    DO_THE_NL_CORR = False\n",
    "    DO_DARK = False\n",
    "    DO_FLAT = False\n",
    "    TIME_BINNING = False\n",
    "\n",
    "    if 'M' in calibration_set:\n",
    "        DO_MASK = True\n",
    "    if 'L' in calibration_set:\n",
    "        DO_THE_NL_CORR = True\n",
    "    if 'D' in calibration_set:\n",
    "        DO_DARK = True\n",
    "    if 'F' in calibration_set:\n",
    "        DO_FLAT = True\n",
    "    if 'B' in calibration_set:\n",
    "        TIME_BINNING = True\n",
    "\n",
    "    cut_inf, cut_sup = 39, 321\n",
    "    l = cut_sup - cut_inf\n",
    "\n",
    "    for n, index_chunk in enumerate(tqdm(index)):\n",
    "        AIRS_CH0_clean = np.zeros((CHUNKS_SIZE, 11250, 32, l))\n",
    "        FGS1_clean = np.zeros((CHUNKS_SIZE, 135000, 32, 32))\n",
    "        \n",
    "        for i in range (CHUNKS_SIZE) : \n",
    "            df = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/AIRS-CH0_signal.parquet'))\n",
    "            signal = df.values.astype(np.float64).reshape((df.shape[0], 32, 356))\n",
    "            gain = train_adc_info['AIRS-CH0_adc_gain'].loc[index_chunk[i]]\n",
    "            offset = train_adc_info['AIRS-CH0_adc_offset'].loc[index_chunk[i]]\n",
    "            signal = ADC_convert(signal, gain, offset)\n",
    "            dt_airs = axis_info['AIRS-CH0-integration_time'].dropna().values\n",
    "            dt_airs[1::2] += 0.1\n",
    "            chopped_signal = signal[:, :, cut_inf:cut_sup]\n",
    "            del signal, df\n",
    "            \n",
    "            # CLEANING THE DATA: AIRS\n",
    "            flat = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/AIRS-CH0_calibration/flat.parquet')).values.astype(np.float64).reshape((32, 356))[:, cut_inf:cut_sup]\n",
    "            dark = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/AIRS-CH0_calibration/dark.parquet')).values.astype(np.float64).reshape((32, 356))[:, cut_inf:cut_sup]\n",
    "            dead_airs = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/AIRS-CH0_calibration/dead.parquet')).values.astype(np.float64).reshape((32, 356))[:, cut_inf:cut_sup]\n",
    "            linear_corr = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/AIRS-CH0_calibration/linear_corr.parquet')).values.astype(np.float64).reshape((6, 32, 356))[:, :, cut_inf:cut_sup]\n",
    "            \n",
    "            if DO_MASK:\n",
    "                chopped_signal = mask_hot_dead(chopped_signal, dead_airs, dark)\n",
    "                AIRS_CH0_clean[i] = chopped_signal\n",
    "            else:\n",
    "                AIRS_CH0_clean[i] = chopped_signal\n",
    "                \n",
    "            if DO_THE_NL_CORR: \n",
    "                linear_corr_signal = apply_linear_corr(linear_corr,AIRS_CH0_clean[i])\n",
    "                AIRS_CH0_clean[i,:, :, :] = linear_corr_signal\n",
    "            del linear_corr\n",
    "            \n",
    "            if DO_DARK: \n",
    "                cleaned_signal = clean_dark(AIRS_CH0_clean[i], dead_airs, dark, dt_airs)\n",
    "                AIRS_CH0_clean[i] = cleaned_signal\n",
    "            else: \n",
    "                pass\n",
    "            del dark\n",
    "            \n",
    "            df = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/FGS1_signal.parquet'))\n",
    "            fgs_signal = df.values.astype(np.float64).reshape((df.shape[0], 32, 32))\n",
    "            \n",
    "            FGS1_gain = train_adc_info['FGS1_adc_gain'].loc[index_chunk[i]]\n",
    "            FGS1_offset = train_adc_info['FGS1_adc_offset'].loc[index_chunk[i]]\n",
    "            \n",
    "            fgs_signal = ADC_convert(fgs_signal, FGS1_gain, FGS1_offset)\n",
    "            dt_fgs1 = np.ones(len(fgs_signal))*0.1\n",
    "            dt_fgs1[1::2] += 0.1\n",
    "            chopped_FGS1 = fgs_signal\n",
    "            del fgs_signal, df\n",
    "            \n",
    "            # CLEANING THE DATA: FGS1\n",
    "            flat = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/FGS1_calibration/flat.parquet')).values.astype(np.float64).reshape((32, 32))\n",
    "            dark = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/FGS1_calibration/dark.parquet')).values.astype(np.float64).reshape((32, 32))\n",
    "            dead_fgs1 = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/FGS1_calibration/dead.parquet')).values.astype(np.float64).reshape((32, 32))\n",
    "            linear_corr = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/FGS1_calibration/linear_corr.parquet')).values.astype(np.float64).reshape((6, 32, 32))\n",
    "            \n",
    "            if DO_MASK:\n",
    "                chopped_FGS1 = mask_hot_dead(chopped_FGS1, dead_fgs1, dark)\n",
    "                FGS1_clean[i] = chopped_FGS1\n",
    "            else:\n",
    "                FGS1_clean[i] = chopped_FGS1\n",
    "\n",
    "            if DO_THE_NL_CORR: \n",
    "                linear_corr_signal = apply_linear_corr(linear_corr,FGS1_clean[i])\n",
    "                FGS1_clean[i,:, :, :] = linear_corr_signal\n",
    "            del linear_corr\n",
    "            \n",
    "            if DO_DARK: \n",
    "                cleaned_signal = clean_dark(FGS1_clean[i], dead_fgs1, dark,dt_fgs1)\n",
    "                FGS1_clean[i] = cleaned_signal\n",
    "            else: \n",
    "                pass\n",
    "            del dark\n",
    "            \n",
    "        # SAVE DATA AND FREE SPACE\n",
    "        AIRS_cds = get_cds(AIRS_CH0_clean)\n",
    "        FGS1_cds = get_cds(FGS1_clean)\n",
    "        \n",
    "        del AIRS_CH0_clean, FGS1_clean\n",
    "        \n",
    "        ## (Optional) Time Binning to reduce space\n",
    "        if TIME_BINNING:\n",
    "            AIRS_cds_binned = bin_obs(AIRS_cds,binning=30)\n",
    "            FGS1_cds_binned = bin_obs(FGS1_cds,binning=30*12)\n",
    "        else:\n",
    "            AIRS_cds = AIRS_cds.transpose(0,1,3,2) ## this is important to make it consistent for flat fielding, but you can always change it\n",
    "            AIRS_cds_binned = AIRS_cds\n",
    "            FGS1_cds = FGS1_cds.transpose(0,1,3,2)\n",
    "            FGS1_cds_binned = FGS1_cds\n",
    "        \n",
    "        del AIRS_cds, FGS1_cds\n",
    "        \n",
    "        for i in range (CHUNKS_SIZE):\n",
    "            flat_airs = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/AIRS-CH0_calibration/flat.parquet')).values.astype(np.float64).reshape((32, 356))[:, cut_inf:cut_sup]\n",
    "            flat_fgs = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/FGS1_calibration/flat.parquet')).values.astype(np.float64).reshape((32, 32))\n",
    "            if DO_FLAT:\n",
    "                corrected_AIRS_cds_binned = correct_flat_field(flat_airs,dead_airs, AIRS_cds_binned[i])\n",
    "                AIRS_cds_binned[i] = corrected_AIRS_cds_binned\n",
    "                corrected_FGS1_cds_binned = correct_flat_field(flat_fgs,dead_fgs1, FGS1_cds_binned[i])\n",
    "                FGS1_cds_binned[i] = corrected_FGS1_cds_binned\n",
    "        np.save(os.path.join(path_out, suffix, 'AIRS_clean_train_{}.npy'.format(n)), AIRS_cds_binned)\n",
    "        np.save(os.path.join(path_out, suffix, 'FGS1_train_{}.npy'.format(n)), FGS1_cds_binned)\n",
    "\n",
    "        del AIRS_cds_binned\n",
    "        del FGS1_cds_binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:10<00:00, 70.73s/it]\n"
     ]
    }
   ],
   "source": [
    "# Example avec tous les paramètres de calibration activés\n",
    "suffix = def_suffix(calibration_set)\n",
    "preprocess(path_out, path_folder, calibration_set, CHUNKS_SIZE)\n",
    "data_train = load_data(os.path.join(path_out, suffix, 'AIRS_clean_train'), CHUNKS_SIZE, len(index))\n",
    "data_train_FGS = load_data(os.path.join(path_out, suffix, 'FGS1_train'), CHUNKS_SIZE, len(index))\n",
    "\n",
    "np.save(os.path.join(path_out, suffix, 'data_train.npy'), data_train)\n",
    "np.save(os.path.join(path_out, suffix, 'data_train_FGS.npy'), data_train_FGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_combinations(elements):\n",
    "    \"\"\"Return a list of all combinations of elements.\"\"\"\n",
    "    all_combinations = []\n",
    "    for r in range(1, len(elements) + 1):\n",
    "        all_combinations.extend([list(comb) for comb in combinations(elements, r)])\n",
    "    return all_combinations\n",
    "\n",
    "all_combinations = get_all_combinations(calibration_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for calibration_set in all_combinations:\n",
    "    suffix = def_suffix(calibration_set)\n",
    "    preprocess(path_out, path_folder, calibration_set, CHUNKS_SIZE)\n",
    "    data_train = load_data(os.path.join(path_out, suffix, 'AIRS_clean_train'), CHUNKS_SIZE, len(index))\n",
    "    data_train_FGS = load_data(os.path.join(path_out, suffix, 'FGS1_train'), CHUNKS_SIZE, len(index))\n",
    "\n",
    "    np.save(os.path.join(path_out, suffix, 'data_train.npy'), data_train)\n",
    "    np.save(os.path.join(path_out, suffix, 'data_train_FGS.npy'), data_train_FGS)\n",
    "    os.remove(os.path.join(path_out, suffix, 'AIRS_clean_train_0.npy'))\n",
    "    os.remove(os.path.join(path_out, suffix, 'FGS1_train_0.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
